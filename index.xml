<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Evan&#39;s Notes</title>
    <link>https://evanap.com/</link>
    <description>Recent content on Evan&#39;s Notes</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 09 Aug 2020 09:26:14 +0700</lastBuildDate>
    
        <atom:link href="https://evanap.com/index.xml" rel="self" type="application/rss+xml" />
    
    
    
        <item>
        <title>Deploying prow</title>
        <link>https://evanap.com/posts/deploying-prow/</link>
        <pubDate>Sun, 09 Aug 2020 09:26:14 +0700</pubDate>
        
        <guid>https://evanap.com/posts/deploying-prow/</guid>
        <description>Evan&#39;s Notes https://evanap.com/posts/deploying-prow/ -&lt;p&gt;Prow is a CI/CD system used in the Kubernetes repositories, and many more. It&amp;rsquo;s known for the ChatOps feature (at least to me) which I think found very cool. I decided to give it a try myself and see it in action.&lt;/p&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;A working Kubernetes cluster with internet access&lt;/li&gt;
&lt;li&gt;A GCP account&lt;/li&gt;
&lt;li&gt;A GitHub account&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;
&lt;p&gt;Prow can be found in Kubernetes&amp;rsquo; &lt;code&gt;test-infra&lt;/code&gt; repository (&lt;a href=&#34;https://github.com/kubernetes/test-infra/tree/master/prow&#34;&gt;here&lt;/a&gt;). The first thing to do is to clone the repo:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone https://github.com/kubernetes/test-infra.git
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;After that we need to set up an access to GitHub for Prow by creating an access token. The required scope are as follows.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;repo:status
public_repo
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Next, we need to create two secrets to be used by Prow. One is &lt;code&gt;hmac-token&lt;/code&gt; that&amp;rsquo;s going to be used to validate webhooks&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;openssl rand -hex 20 &amp;gt; hmac-token
kubetl create secret generic hmac-token --from-file=hmac=./hmac-token
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;And the other is the &lt;code&gt;github-token&lt;/code&gt; for the bot account&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;echo &#39;insert-github-token-here&#39; &amp;gt; github-token
kubetl create secret generic github-token --from-file=token=./github-token
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Next, we&amp;rsquo;re going to update the sample manifest that we&amp;rsquo;re going to use. There are two sample manifests, one with minio as blog storage (&lt;a href=&#34;https://github.com/kubernetes/test-infra/blob/master/config/prow/cluster/starter-s3.yaml&#34;&gt;starter-s3.yaml&lt;/a&gt;) and the other one that uses GCS (&lt;a href=&#34;https://github.com/kubernetes/test-infra/blob/master/config/prow/cluster/starter-gcs.yaml&#34;&gt;starter-gcs.yaml&lt;/a&gt;). I decided to use the one with GCS.&lt;/p&gt;
&lt;p&gt;With both sample manifests there are things that we need to adjust, those are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GitHub token, replace &lt;code&gt;&amp;lt;&amp;lt;insert-token-here&amp;gt;&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;hmac token, replace &lt;code&gt;&amp;lt;&amp;lt; insert-hmac-token-here &amp;gt;&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;domain, replace &lt;code&gt;&amp;lt;&amp;lt; your-domain.com &amp;gt;&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;GitHub organization, replace &lt;code&gt;&amp;lt;&amp;lt; your_github_org &amp;gt;&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Prow artifacts bucket, replace &lt;code&gt;gs://bucket-name&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Tide&amp;rsquo;s GCS bucket, replace &lt;code&gt;gs://tide/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Statusreconciler&amp;rsquo;s GCS bucket, replace &lt;code&gt;gs://status-reconciler/&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then we need to provide the buckets and a service account to be used by Prow:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ gcloud iam service-accounts create prow-gcs-publisher
$ identifier=&amp;quot;$(gcloud iam service-accounts list --filter &#39;name:prow-gcs-publisher&#39; --format &#39;value(email)&#39;)&amp;quot;
$ gsutil mb gs://prow-artifacts-bucket-name/ 
$ gsutil mb gs://tide-bucket-name/ 
$ gsutil mb gs://status-reconciler-bucket-name/ 
$ gsutil iam ch allUsers:objectViewer gs://prow-artifacts-bucket-name 
$ gsutil iam ch allUsers:objectViewer gs://tide-bucket-name
$ gsutil iam ch allUsers:objectViewer gs://status-reconciler-bucket-name
$ gsutil iam ch &amp;quot;serviceAccount:${identifier}:objectAdmin&amp;quot; gs://prow-artifacts 
$ gsutil iam ch &amp;quot;serviceAccount:${identifier}:objectAdmin&amp;quot; gs://tide-bucket-name  
$ gsutil iam ch &amp;quot;serviceAccount:${identifier}:objectAdmin&amp;quot; gs://status-reconciler-bucket-name
$ gcloud iam service-accounts keys create --iam-account &amp;quot;${identifier}&amp;quot; service-account.json 
$ kubectl -n test-pods create secret generic gcs-credentials --from-file=service-account.json 
$ kubectl -n prow create secret generic gcs-credentials --from-file=service-account.json 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Next is adding Prow components to the cluster:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl apply -f config/prow/cluster/starter-gcs.yaml&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;If things goes well we should be able to our external address.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl get -n prow ingress prow
NAME   CLASS    HOSTS                     ADDRESS               	PORTS     AGE
prow   &amp;lt;none&amp;gt;   prow.&amp;lt;&amp;lt;yourdomain.com&amp;gt;&amp;gt;   an.ip.addr.ess          80, 443   22d
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Once we get the address we can go to the address to see Prow&amp;rsquo;s deck. We should see that there&amp;rsquo;s an &amp;ldquo;echo-test&amp;rdquo; job, if it is completed there&amp;rsquo;d be a green check-mark next to it which means Prow is ready to serve.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/evanap/evanap.github.io/raw/hugo-site/images/deploying-prow-1.png&#34; alt=&#34;alt text&#34; title=&#34;The echo-test job(s) completed&#34;&gt;&lt;/p&gt;
&lt;p&gt;Next thing we need to do is to add the webhook to GitHub:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Go to repo and click &lt;code&gt;Settings -&amp;gt; Webhooks&lt;/code&gt;, and click &lt;code&gt;Add webhook&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Use the address we get before as the &lt;code&gt;Payload URL&lt;/code&gt;, it should be something like this &lt;code&gt;http://an.ip.addr.ess/hook&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Change the &lt;code&gt;Content type&lt;/code&gt; to &lt;code&gt;application/json&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Use the &lt;code&gt;hmac-token&lt;/code&gt; we generated before as the &lt;code&gt;Secret&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Change the trigger to &lt;code&gt;Send me everything&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Click &lt;code&gt;Add webhook&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We should be all set with that.&lt;/p&gt;
&lt;h2 id=&#34;usage&#34;&gt;Usage&lt;/h2&gt;
&lt;p&gt;Didn&amp;rsquo;t I say that Prow is known for ChatOps? Let&amp;rsquo;s take a look into that. Create a new PR in the repo that&amp;rsquo;s set up before, and try using the commands &lt;code&gt;/meow&lt;/code&gt; or &lt;code&gt;/woof&lt;/code&gt; to send some fluffs to our PR.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/evanap/evanap.github.io/raw/hugo-site/images/deploying-prow-2.png&#34; alt=&#34;alt text&#34; title=&#34;Look at them meow&#34;&gt;&lt;/p&gt;
&lt;p&gt;Cool, right? There&amp;rsquo;s more to that. We can see the list of plugins available for Prow by visiting our Prow&amp;rsquo;s plugins catalog.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;http://prow.&amp;lt;&amp;lt; your-domain.com &amp;gt;&amp;gt;/plugins
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://github.com/evanap/evanap.github.io/raw/hugo-site/images/deploying-prow-3.png&#34; alt=&#34;alt text&#34; title=&#34;Prow plugins&#34;&gt;&lt;/p&gt;
&lt;p&gt;Should you find interesting plugins you want to use or unused plugins to remove, simply edit the &lt;code&gt;prow&lt;/code&gt; ConfigMap and add or remove the plugins.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: ConfigMap
metadata:
  namespace: prow
  name: plugins
data:
  plugins.yaml: |
    plugins:
      evanap/hello:
      - approve
      - assign
      - blunderbuss
      - cat
      - dog
      - help
      - heart
      - hold
      - label
      - goose # honks the PR
      - override
      - shrug
      - owners-label
      - lifecycle
      - lgtm
      - trigger
      - verify-owners
      - wip
      - yuks
&lt;/code&gt;&lt;/pre&gt;- https://evanap.com/posts/deploying-prow/ - </description>
        </item>
    
    
    
        <item>
        <title>Using awsudo</title>
        <link>https://evanap.com/posts/using-awsudo/</link>
        <pubDate>Sat, 08 Aug 2020 11:57:00 +0700</pubDate>
        
        <guid>https://evanap.com/posts/using-awsudo/</guid>
        <description>Evan&#39;s Notes https://evanap.com/posts/using-awsudo/ -&lt;p&gt;Recently I&amp;rsquo;ve been using this tool called &lt;code&gt;awsudo&lt;/code&gt;. It&amp;rsquo;s a simple tool that helps you assume roles if you use that strategy in AWS.&lt;/p&gt;
&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;
&lt;p&gt;Run this command to start the installation:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;bash &amp;lt;(curl https://raw.githubusercontent.com/makethunder/awsudo/master/install)
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;usage&#34;&gt;Usage&lt;/h2&gt;
&lt;p&gt;If this is your first time using AWS cli, configure your credentials first:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ aws configure
AWS Access Key ID &lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;None&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt;: insert-your-access-key-id
AWS Secret Access Key &lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;None&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt;: insert-your-secret-access-key
Default region name &lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;None&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt;: ap-southeast-1
Default output format &lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;None&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt;: json
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now that you have your credentials configured, you can start using &lt;code&gt;awsudo&lt;/code&gt;. I use it to switch between roles I have set up in &lt;code&gt;~/.aws/config&lt;/code&gt;, it looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[profile infra]
role_arn = arn:aws:iam::123456789012:role/infra
source_profile = default
region = ap-southeast-1
mfa_serial = arn:aws:iam::98765432100:mfa/evan
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If you don&amp;rsquo;t use MFA to assume roles you can remove the last line, but I recommend using MFA for extra layer of security. Now you can start assuming roles you have defined by running:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ awsudo -u infra env | grep AWS
&lt;span style=&#34;color:#19177c&#34;&gt;AWS_SESSION_TOKEN&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;AQoDYXdzEBcaoAKIYnZ67+8/BzPkkpbpR3yfv9bAQoDYXdzEBcaoAKIYnZ67+8/BzPkkpbpR3yfv9b
&lt;span style=&#34;color:#19177c&#34;&gt;AWS_SECRET_ACCESS_KEY&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;rkCLOMJMx2DbGoGySIETU8aRFfjGxgJAzDJ6Zt+3
&lt;span style=&#34;color:#19177c&#34;&gt;AWS_ACCESS_KEY_ID&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;AKIAIXAKX3ABKZACKEDN
&lt;span style=&#34;color:#19177c&#34;&gt;AWS_DEFAULT_REGION&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;ap-southeast-1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The command outputs the credentials required to run things using AWS cli, but in at this state you would need to run it every time you want to invoke AWS cli. I made it simpler by exporting those credentials to the current bash session:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#008000&#34;&gt;eval&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;$(&lt;/span&gt;sed &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;s/^/export /&amp;#39;&lt;/span&gt; &amp;lt;&lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;awsudo -u infra env | grep AWS&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;)&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;)&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now you just need to run it once every time your credentials expired.&lt;/p&gt;
- https://evanap.com/posts/using-awsudo/ - </description>
        </item>
    
    
    
        <item>
        <title>Vertical Pod Autoscaler Recommendation</title>
        <link>https://evanap.com/posts/vertical-pod-autoscaler-recommendation/</link>
        <pubDate>Sun, 16 Feb 2020 00:00:00 +0000</pubDate>
        
        <guid>https://evanap.com/posts/vertical-pod-autoscaler-recommendation/</guid>
        <description>Evan&#39;s Notes https://evanap.com/posts/vertical-pod-autoscaler-recommendation/ -&lt;p&gt;Right-sizing a deployment of an app on a Kubernetes clulster is a tricky thing to do. Setting requests and limits too high, you might end up with unschedullable pods due to resource scarcity. Setting requets and limits too low and you might end up with OOMKilled pods. Luckily, there&amp;rsquo;s an answer (kind of) for that. It&amp;rsquo;s the &lt;strong&gt;Vertical Pod Autoscaler&lt;/strong&gt; or &lt;strong&gt;VPA&lt;/strong&gt; for short.&lt;/p&gt;
&lt;p&gt;VPA is a custom resource, a part of autoscaling components of Kubernetes. It can scale resource and limits of the containers in pods, based on their usage over time. Very neat.&lt;/p&gt;
&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;
&lt;p&gt;s
Installation of the VPA is pretty straightforward. You just need to clone the repo &lt;a href=&#34;https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler&#34;&gt;here&lt;/a&gt; and run the provisioning script:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone https://github.com/kubernetes/autoscaler.git
cd autoscaler/veretical-pod-autoscaler
./hack/vpa-up.sh
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;em&gt;Note: you need cluster-admin role to provision the VPA system components&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;usage&#34;&gt;Usage&lt;/h2&gt;
&lt;p&gt;If you&amp;rsquo;ve used the Horizontal Pod Autoscaler (HPA) in the past, then using the VPA is not much different. You just need to create a VPA object that points to your target deployment. Below is an example of a VPA definition:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;---&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;apiVersion&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;autoscaling.k8s.io/v1beta2&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;kind&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;VerticalPodAutoscaler&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;metadata&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;your-app-vpa&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;namespace&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;default&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;spec&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;targetRef&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;apiVersion&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;apps/v1&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;kind&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Deployment&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;your-app&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;updatePolicy&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;updateMode&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Off&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In the example I define the &lt;strong&gt;updateMode&lt;/strong&gt; of the VPA. Currently, there are three available update modes of the VPA:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Auto: Makes updates directly to your pods&lt;/li&gt;
&lt;li&gt;Recreate: Evict old pods and create new ones with updated definition&lt;/li&gt;
&lt;li&gt;Off: Not applying any recommendations&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For me, setting the update mode &lt;code&gt;Off&lt;/code&gt; is the safest way to use VPA since there&amp;rsquo;s no modification taking place until we apply the recommendation ourselves.&lt;/p&gt;
&lt;p&gt;If the VPA is successfully created, after about 5 minutes you will start getting resource recommendation of the targeted deployment based on its usage history. You can grab the recommendation by describing the VPA:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl describe vpa your-app-vpa
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;And it will look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Name:         your-app-vpa
Namespace:    default
Labels:       &amp;lt;none&amp;gt;
Annotations:  kubectl.kubernetes.io/last-applied-configuration:
                {&amp;quot;apiVersion&amp;quot;:&amp;quot;autoscaling.k8s.io/v1beta2&amp;quot;,&amp;quot;kind&amp;quot;:&amp;quot;VerticalPodAutoscaler&amp;quot;,&amp;quot;metadata&amp;quot;:{&amp;quot;annotations&amp;quot;:{},&amp;quot;name&amp;quot;:&amp;quot;your-app-vpa&amp;quot;,&amp;quot;namespace&amp;quot;:&amp;quot;d...
API Version:  autoscaling.k8s.io/v1beta2
Kind:         VerticalPodAutoscaler
Metadata:
  Creation Timestamp:  2020-02-16T14:04:28Z
  Generation:          29
  Resource Version:    8472916
  Self Link:           /apis/autoscaling.k8s.io/v1beta2/namespaces/default/verticalpodautoscalers/your-app-vpa
  UID:                 086f993f-09b7-461d-ba50-7f2cedf2e95b
Spec:
  Target Ref:
    API Version:  apps/v1
    Kind:         Deployment
    Name:         your-app-vpa
  Update Policy:
    Update Mode:  Off
Status:
  Conditions:
    Last Transition Time:  2020-02-16T14:06:38Z
    Status:                True
    Type:                  RecommendationProvided
  Recommendation:
    Container Recommendations:
      Container Name:  your-app
      Lower Bound:
        Cpu:     556m
        Memory:  131072k
      Target:
        Cpu:     587m
        Memory:  131072k
      Uncapped Target:
        Cpu:     587m
        Memory:  131072k
      Upper Bound:
        Cpu:           17347m
        Memory:        318166666
      Container Name:  istio-proxy
      Lower Bound:
        Cpu:     12m
        Memory:  173661513
      Target:
        Cpu:     12m
        Memory:  183046954
      Uncapped Target:
        Cpu:     12m
        Memory:  183046954
      Upper Bound:
        Cpu:     304m
        Memory:  5064299060
Events:          &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now that we&amp;rsquo;ve got the recommendation provided by the VPA, we can set our deployment resource to not get below the &lt;strong&gt;Lower Bound&lt;/strong&gt; and maybe set it around the recommended &lt;strong&gt;Target&lt;/strong&gt; values.&lt;/p&gt;
&lt;h2 id=&#34;problems&#34;&gt;Problems&lt;/h2&gt;
&lt;p&gt;On my time experimenting with the VPA I faced one weird issue. One VPA in the cluster reports that it has &amp;ldquo;No pods match this VPA object.&amp;rdquo; Given that, it&amp;rsquo;s still giving correct recommendations for the containers of the target deployment.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Name:         server-app-vpa
Namespace:    core
Labels:       &amp;lt;none&amp;gt;
Annotations:  kubectl.kubernetes.io/last-applied-configuration:
                {&amp;quot;apiVersion&amp;quot;:&amp;quot;autoscaling.k8s.io/v1beta2&amp;quot;,&amp;quot;kind&amp;quot;:&amp;quot;VerticalPodAutoscaler&amp;quot;,&amp;quot;metadata&amp;quot;:{&amp;quot;annotations&amp;quot;:{},&amp;quot;name&amp;quot;:&amp;quot;server-app-vpa&amp;quot;,&amp;quot;namespace...
API Version:  autoscaling.k8s.io/v1beta2
Kind:         VerticalPodAutoscaler
Metadata:
  Creation Timestamp:  2020-02-16T14:21:19Z
  Generation:          11
  Resource Version:    8472225
  Self Link:           /apis/autoscaling.k8s.io/v1beta2/namespaces/core/verticalpodautoscalers/server-app-vpa
  UID:                 c0e41569-2216-41cb-abe8-febf4017460b
Spec:
  Target Ref:
    API Version:  apps/v1
    Kind:         Deployment
    Name:         server-app
  Update Policy:
    Update Mode:  Off
Status:
  Conditions:
    Last Transition Time:  2020-02-16T14:21:38Z
    Message:               No pods match this VPA object
    Reason:                NoPodsMatched
    Status:                True
    Type:                  NoPodsMatched
    Last Transition Time:  2020-02-16T14:21:38Z
    Status:                True
    Type:                  RecommendationProvided
  Recommendation:
    Container Recommendations:
      Container Name:  server-app
      Lower Bound:
        Cpu:     70m
        Memory:  1190888880
      Target:
        Cpu:     163m
        Memory:  1312092764
      Uncapped Target:
        Cpu:     163m
        Memory:  1312092764
      Upper Bound:
        Cpu:           10283m
        Memory:        66464285183
      Container Name:  istio-proxy
      Lower Bound:
        Cpu:     20m
        Memory:  184882685
      Target:
        Cpu:     109m
        Memory:  203699302
      Uncapped Target:
        Cpu:     109m
        Memory:  203699302
      Upper Bound:
        Cpu:     6382m
        Memory:  10318423263
Events:          &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The VPA is a very useful feature and I think it can bring a whole lot of optimizations to our Kubernetes clusters. Though, for now it still need some improvements and to get rid of the bugs like the one I mentioned before. Maybe if it matured a bit more we will see use cases of it in a production environment. For now, I&amp;rsquo;m keeping it for staging environments and maybe combine it with traffic shadowing to get a grasp of recommendation without using it in production.&lt;/p&gt;
- https://evanap.com/posts/vertical-pod-autoscaler-recommendation/ - </description>
        </item>
    
    
    
        <item>
        <title>First Post</title>
        <link>https://evanap.com/posts/my-first-post/</link>
        <pubDate>Sat, 15 Feb 2020 18:57:20 +0700</pubDate>
        
        <guid>https://evanap.com/posts/my-first-post/</guid>
        <description>Evan&#39;s Notes https://evanap.com/posts/my-first-post/ -&lt;p&gt;Well, for quite some time I&amp;rsquo;ve been looking to put this personal site of my own. Been stuck in choosing which platform will host this site, whether it&amp;rsquo;s building the platform myself maybe using Docker or some single node Kubernetes cluster, or just a simple web server hosted by a single instance on some cloud. But, the choice comes down to GitHub Pages. It&amp;rsquo;s easy and hassle-free. So, yeah, there&amp;rsquo;s that.&lt;/p&gt;
- https://evanap.com/posts/my-first-post/ - </description>
        </item>
    
    
    
        <item>
        <title>About Me</title>
        <link>https://evanap.com/about/</link>
        <pubDate>Mon, 01 Jan 0001 01:01:00 +0700</pubDate>
        
        <guid>https://evanap.com/about/</guid>
        <description>Evan&#39;s Notes https://evanap.com/about/ -&lt;p&gt;I&amp;rsquo;m Evan. I&amp;rsquo;m an engineer currently interested in infrastructure, cloud, and DevOps.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m used to Terraform and Ansible. I also have experience with Kubernetes and Docker, which picks my interest the most now. I usually wander around the cloud technology provided by Google and Amazon.&lt;/p&gt;
&lt;p&gt;I write YAML files for a living. In my free time I&amp;rsquo;m learning programming in Go or Python. If I get bored I usually hop into my PS4 to play some games or watch anime.&lt;/p&gt;
- https://evanap.com/about/ - </description>
        </item>
    
    
  </channel>
</rss> 